errors:
  UnprocessableEntityError:
    status-code: 422
    type: HttpValidationError
    docs: Validation Error
types:
  BooleanEvaluatorVersionStats:
    docs: |-
      Base attributes for stats for an Evaluator Version-Evaluated Version pair
      in the Evaluation Report.
    properties:
      evaluator_version_id:
        type: string
        docs: Unique identifier for the Evaluator Version.
      total_logs:
        type: integer
        docs: >-
          The total number of Logs generated by this Evaluator Version on the
          Evaluated Version's Logs. This includes Nulls and Errors.
      num_judgments:
        type: integer
        docs: >-
          The total number of Evaluator judgments for this Evaluator Version.
          This excludes Nulls and Errors.
      num_nulls:
        type: integer
        docs: >-
          The total number of null judgments (i.e. abstentions) for this
          Evaluator Version.
      num_errors:
        type: integer
        docs: The total number of errored Evaluators for this Evaluator Version.
      num_true:
        type: integer
        docs: The total number of `True` judgments for this Evaluator Version.
      num_false:
        type: integer
        docs: The total number of `False` judgments for this Evaluator Version.
    source:
      openapi: openapi/openapi.auto.json
  ChatMessageContentItem:
    discriminated: false
    union:
      - TextChatContent
      - ImageChatContent
    source:
      openapi: openapi/openapi.auto.json
  ChatMessageContent:
    discriminated: false
    docs: The content of the message.
    union:
      - string
      - list<ChatMessageContentItem>
    source:
      openapi: openapi/openapi.auto.json
  ChatMessage:
    properties:
      content:
        type: optional<ChatMessageContent>
        docs: The content of the message.
      name:
        type: optional<string>
        docs: Optional name of the message author.
      tool_call_id:
        type: optional<string>
        docs: Tool call that this message is responding to.
      role:
        type: ChatRole
        docs: Role of the message author.
      tool_calls:
        type: optional<list<ToolCall>>
        docs: A list of tool calls requested by the assistant.
    source:
      openapi: openapi/openapi.auto.json
  ChatRole:
    enum:
      - user
      - assistant
      - system
      - tool
    docs: An enumeration.
    source:
      openapi: openapi/openapi.auto.json
  CodeEvaluatorRequest:
    properties:
      arguments_type:
        type: EvaluatorArgumentsType
        docs: Whether this evaluator is target-free or target-required.
      return_type:
        type: EvaluatorReturnTypeEnum
        docs: The type of the return value of the evaluator.
      evaluator_type: literal<"python">
      code:
        type: string
        docs: >-
          The code for the evaluator. This code will be executed in a sandboxed
          environment.
    source:
      openapi: openapi/openapi.auto.json
  CommitRequest:
    properties:
      commit_message:
        type: string
        docs: Message describing the changes made.
    source:
      openapi: openapi/openapi.auto.json
  CreateDatapointRequestTargetValue:
    discriminated: false
    union:
      - string
      - integer
      - double
      - boolean
      - list<unknown>
      - map<string, unknown>
    source:
      openapi: openapi/openapi.auto.json
  CreateDatapointRequest:
    properties:
      inputs:
        type: optional<map<string, string>>
        docs: The inputs to the prompt template.
      messages:
        type: optional<list<ChatMessage>>
        docs: List of chat messages to provide to the model.
      target:
        type: optional<map<string, CreateDatapointRequestTargetValue>>
        docs: >-
          Object with criteria necessary to evaluate generations with this
          Datapoint. This is passed in as an argument to Evaluators when used in
          an Evaluation.
    source:
      openapi: openapi/openapi.auto.json
  CreateEvaluationRequest:
    docs: >-
      Request model for creating an Evaluation.


      Evaluation benchmark your Prompt/Tool Versions. With the Datapoints in a
      Dataset Version,

      Logs corresponding to the Datapoint and each Evaluated Version are
      evaluated by the specified Evaluator Versions.

      Aggregated statistics are then calculated and presented in the Evaluation.
    properties:
      dataset:
        type: EvaluationsDatasetRequest
        docs: The Dataset Version to use in this Evaluation.
      evaluatees:
        type: optional<list<EvaluateeRequest>>
        docs: >-
          Unique identifiers for the Prompt/Tool Versions to include in the
          Evaluation Report. Can be left unpopulated if you wish to add
          evaluatees to this Evaluation Report by specifying `evaluation_id` in
          Log calls.
      evaluators:
        docs: The Evaluators used to evaluate.
        type: list<EvaluationsRequest>
    source:
      openapi: openapi/openapi.auto.json
  CreateEvaluatorLogResponse:
    properties:
      id:
        type: string
        docs: String identifier of the new Log.
      parent_id:
        type: string
        docs: Identifier of the evaluated parent Log.
      session_id:
        type: optional<string>
        docs: >-
          Identifier of the Session containing both the parent and the new child
          Log. If the parent Log does not belong to a Session, a new Session is
          created with this ID.
      version_id:
        type: string
        docs: Identifier of Evaluator Version for which the Log was registered.
    source:
      openapi: openapi/openapi.auto.json
  CreatePromptLogResponse:
    properties:
      id:
        type: string
        docs: String ID of log.
      prompt_id:
        type: string
        docs: ID of the Prompt the log belongs to.
      version_id:
        type: string
        docs: ID of the specific version of the Prompt.
      session_id:
        type: optional<string>
        docs: String ID of session the log belongs to.
    source:
      openapi: openapi/openapi.auto.json
  CreateToolLogResponse:
    properties:
      id:
        type: string
        docs: String ID of log.
      tool_id:
        type: string
        docs: ID of the Tool the log belongs to.
      version_id:
        type: string
        docs: ID of the specific version of the Tool.
      session_id:
        type: optional<string>
        docs: String ID of session the log belongs to.
    source:
      openapi: openapi/openapi.auto.json
  DashboardConfiguration:
    properties:
      time_unit: TimeUnit
      time_range_days: integer
      model_config_ids: list<string>
    source:
      openapi: openapi/openapi.auto.json
  DatapointResponseTargetValue:
    discriminated: false
    union:
      - string
      - integer
      - double
      - boolean
      - list<unknown>
      - map<string, unknown>
    source:
      openapi: openapi/openapi.auto.json
  DatapointResponse:
    properties:
      inputs:
        type: optional<map<string, string>>
        docs: The inputs to the prompt template.
      messages:
        type: optional<list<ChatMessage>>
        docs: List of chat messages to provide to the model.
      target:
        type: optional<map<string, DatapointResponseTargetValue>>
        docs: >-
          Object with criteria necessary to evaluate generations with this
          Datapoint. This is passed in as an argument to Evaluators when used in
          an Evaluation.
      id:
        type: string
        docs: Unique identifier for the Datapoint. Starts with `dp_`.
    source:
      openapi: openapi/openapi.auto.json
  DatasetResponse:
    docs: >-
      Base type that all File Responses should inherit from.


      Attributes defined here are common to all File Responses and should be
      overridden

      in the inheriting classes with documentation and appropriate Field
      definitions.
    properties:
      path:
        type: string
        docs: >-
          Path of the Dataset, including the name, which is used as a unique
          identifier.
      id:
        type: string
        docs: Unique identifier for the Dataset. Starts with `ds_`.
      directory_id:
        type: optional<string>
        docs: ID of the directory that the file is in on Humanloop.
      name:
        type: string
        docs: Name of the Dataset, which is used as a unique identifier.
      version_id:
        type: string
        docs: >-
          Unique identifier for the specific Dataset Version. If no query params
          provided, the default deployed Dataset Version is returned. Starts
          with `dsv_`.
      type: optional<literal<"dataset">>
      environments:
        type: optional<list<EnvironmentResponse>>
        docs: The list of environments the Dataset Version is deployed to.
      created_at: datetime
      updated_at: datetime
      created_by:
        type: optional<UserResponse>
        docs: The user who created the Dataset.
      status:
        type: VersionStatus
        docs: The status of the Dataset Version.
      last_used_at: datetime
      commit_message:
        type: optional<string>
        docs: >-
          Message describing the changes made. If provided, a committed version
          of the Dataset is created. Otherwise, an uncommitted version is
          created.
      datapoints_count:
        type: integer
        docs: The number of Datapoints in this Dataset version.
      datapoints:
        type: optional<list<DatapointResponse>>
        docs: >-
          The list of Datapoints in this Dataset version. Only provided if
          explicitly requested.
    source:
      openapi: openapi/openapi.auto.json
  EnvironmentResponse:
    properties:
      id: string
      created_at: datetime
      name: string
      tag: EnvironmentTag
    source:
      openapi: openapi/openapi.auto.json
  EnvironmentTag:
    enum:
      - default
      - other
    docs: An enumeration.
    source:
      openapi: openapi/openapi.auto.json
  EvaluatedVersionResponse:
    discriminated: false
    union:
      - PromptResponse
      - ToolResponse
      - EvaluatorResponse
    source:
      openapi: openapi/openapi.auto.json
  EvaluateeRequest:
    docs: |-
      Specification of a File version on Humanloop.

      This can be done in a couple of ways:
      - Specifying `version_id` directly.
      - Specifying a File (and optionally an Environment).
          - A File can be specified by either `path` or `file_id`.
          - An Environment can be specified by `environment_id`. If no Environment is specified, the default Environment is used.
    properties:
      version_id:
        type: optional<string>
        docs: >-
          Unique identifier for the File Version. If provided, none of the other
          fields should be specified.
      path:
        type: optional<string>
        docs: >-
          Path identifying a File. Provide either this or `file_id` if you want
          to specify a File.
      file_id:
        type: optional<string>
        docs: >-
          Unique identifier for the File. Provide either this or `path` if you
          want to specify a File.
      environment:
        type: optional<string>
        docs: >-
          Name of the Environment a Version is deployed to. Only provide this
          when specifying a File. If not provided (and a File is specified), the
          default Environment is used.
      batch_id:
        type: optional<string>
        docs: >-
          Unique identifier for the batch of Logs to include in the Evaluation
          Report.
      orchestrated:
        type: optional<boolean>
        docs: >-
          Whether the Prompt/Tool is orchestrated by Humanloop. Default is
          `True`. If `False`, a log for the Prompt/Tool should be submitted by
          the user via the API.
        default: true
    source:
      openapi: openapi/openapi.auto.json
  EvaluateeResponse:
    docs: Version of the Evaluatee being evaluated.
    properties:
      version: EvaluatedVersionResponse
      batch_id:
        type: optional<string>
        docs: >-
          Unique identifier for the batch of Logs to include in the Evaluation
          Report. 
      orchestrated:
        type: boolean
        docs: >-
          Whether the Prompt/Tool is orchestrated by Humanloop. Default is
          `True`. If `False`, a log for the Prompt/Tool should be submitted by
          the user via the API.
    source:
      openapi: openapi/openapi.auto.json
  EvaluationEvaluatorResponse:
    properties:
      version: EvaluatorResponse
      orchestrated:
        type: boolean
        docs: >-
          Whether the Evaluator is orchestrated by Humanloop. Default is `True`.
          If `False`, a log for the Evaluator should be submitted by the user
          via the API.
    source:
      openapi: openapi/openapi.auto.json
  EvaluationReportLogResponse:
    properties:
      evaluated_version:
        type: EvaluatedVersionResponse
        docs: The version of the Prompt, Tool or Evaluator that the Log belongs to.
      datapoint:
        type: DatapointResponse
        docs: The Datapoint used to generate the Log
      log:
        type: optional<LogResponse>
        docs: The Log that was evaluated by the Evaluator.
      evaluator_logs:
        docs: The Evaluator Logs containing the judgments for the Log.
        type: list<LogResponse>
    source:
      openapi: openapi/openapi.auto.json
  EvaluationResponse:
    properties:
      id:
        type: string
        docs: Unique identifier for the Evaluation. Starts with `evr`.
      dataset:
        type: DatasetResponse
        docs: The Dataset used in the Evaluation.
      evaluatees:
        docs: The Prompt/Tool Versions included in the Evaluation.
        type: list<EvaluateeResponse>
      evaluators:
        docs: The Evaluator Versions used to evaluate.
        type: list<EvaluationEvaluatorResponse>
      status:
        type: EvaluationStatus
        docs: >
          The current status of the Evaluation.


          - `"pending"`: The Evaluation has been created but is not actively
          being worked on by Humanloop.

          - `"running"`: Humanloop is checking for any missing Logs and
          Evaluator Logs, and will generate them where appropriate.

          - `"completed"`: All Logs an Evaluator Logs have been generated.

          - `"cancelled"`: The Evaluation has been cancelled by the user.
          Humanloop will stop generating Logs and Evaluator Logs.
      created_at: datetime
      created_by: optional<UserResponse>
      updated_at: datetime
      url:
        type: optional<string>
        docs: URL to view the Evaluation on the Humanloop.
    source:
      openapi: openapi/openapi.auto.json
  EvaluationStats:
    properties:
      overall_stats:
        type: OverallStats
        docs: Stats for the Evaluation Report as a whole.
      version_stats:
        docs: Stats for each Evaluated Version in the Evaluation Report.
        type: list<VersionStats>
    source:
      openapi: openapi/openapi.auto.json
  EvaluationStatus:
    enum:
      - pending
      - running
      - completed
      - cancelled
    docs: Status of an evaluation.
    source:
      openapi: openapi/openapi.auto.json
  EvaluatorActivationDeactivationRequestActivateItem:
    discriminated: false
    union:
      - MonitoringEvaluatorVersionRequest
      - MonitoringEvaluatorEnvironmentRequest
    source:
      openapi: openapi/openapi.auto.json
  EvaluatorActivationDeactivationRequestDeactivateItem:
    discriminated: false
    union:
      - MonitoringEvaluatorVersionRequest
      - MonitoringEvaluatorEnvironmentRequest
    source:
      openapi: openapi/openapi.auto.json
  EvaluatorActivationDeactivationRequest:
    properties:
      activate:
        type: optional<list<EvaluatorActivationDeactivationRequestActivateItem>>
        docs: >-
          Evaluators to activate for Monitoring. These will be automatically run
          on new Logs.
      deactivate:
        type: optional<list<EvaluatorActivationDeactivationRequestDeactivateItem>>
        docs: Evaluators to deactivate. These will not be run on new Logs.
    source:
      openapi: openapi/openapi.auto.json
  EvaluatorAggregate:
    properties:
      value:
        type: double
        docs: The aggregated value of the evaluator.
      evaluator_id:
        type: string
        docs: ID of the evaluator.
      evaluator_version_id:
        type: string
        docs: ID of the evaluator version.
      created_at: datetime
      updated_at: datetime
    source:
      openapi: openapi/openapi.auto.json
  EvaluatorArgumentsType:
    enum:
      - target_free
      - target_required
    docs: Enum representing the possible argument types of an evaluator.
    source:
      openapi: openapi/openapi.auto.json
  EvaluatorJudgmentNumberLimit:
    properties:
      min:
        type: optional<double>
        docs: The minimum value that can be selected.
      max:
        type: optional<double>
        docs: The maximum value that can be selected.
      step:
        type: optional<double>
        docs: The step size for the number input.
    source:
      openapi: openapi/openapi.auto.json
  EvaluatorJudgmentOptionResponse:
    properties:
      name:
        type: string
        docs: The name of the option.
      valence:
        type: optional<Valence>
        docs: Whether this option should be considered positive or negative.
    source:
      openapi: openapi/openapi.auto.json
  EvaluatorLogResponseJudgment:
    discriminated: false
    docs: Evaluator assessment of the Log.
    union:
      - boolean
      - string
      - list<string>
      - double
    source:
      openapi: openapi/openapi.auto.json
  EvaluatorLogResponse:
    docs: General request for creating a Log
    properties:
      output:
        type: optional<string>
        docs: >-
          Generated output from your model for the provided inputs. Can be
          `None` if logging an error, or if creating a parent Log with the
          intention to populate it later.
      created_at:
        type: optional<datetime>
        docs: 'User defined timestamp for when the log was created. '
      error:
        type: optional<string>
        docs: Error message if the log is an error.
      provider_latency:
        type: optional<double>
        docs: Duration of the logged event in seconds.
      stdout:
        type: optional<string>
        docs: Captured log and debug statements.
      provider_request:
        type: optional<map<string, unknown>>
        docs: Raw request sent to provider.
      provider_response:
        type: optional<map<string, unknown>>
        docs: Raw response received the provider.
      session_id:
        type: optional<string>
        docs: >-
          Unique identifier for the Session to associate the Log to. Allows you
          to record multiple Logs to a Session (using an ID kept by your
          internal systems) by passing the same `session_id` in subsequent log
          requests. 
      parent_id:
        type: optional<string>
        docs: >-
          Identifier of the evaluated Log. The newly created Log will have this
          one set as parent.
      inputs:
        type: optional<map<string, unknown>>
        docs: The inputs passed to the prompt template.
      source:
        type: optional<string>
        docs: Identifies where the model was called from.
      metadata:
        type: optional<map<string, unknown>>
        docs: Any additional metadata to record.
      save:
        type: optional<boolean>
        docs: Whether the request/response payloads will be stored on Humanloop.
        default: true
      source_datapoint_id:
        type: optional<string>
        docs: >-
          Unique identifier for the Datapoint that this Log is derived from.
          This can be used by Humanloop to associate Logs to Evaluations. If
          provided, Humanloop will automatically associate this Log to
          Evaluations that require a Log for this Datapoint-Version pair.
      batches:
        type: optional<list<string>>
        docs: >-
          Array of Batch Ids that this log is part of. Batches are used to group
          Logs together for offline Evaluations
      user:
        type: optional<string>
        docs: End-user ID related to the Log.
      environment:
        type: optional<string>
        docs: The name of the Environment the Log is associated to.
      judgment:
        type: optional<EvaluatorLogResponseJudgment>
        docs: Evaluator assessment of the Log.
      id:
        type: string
        docs: Unique identifier for the Log.
      evaluator_logs:
        docs: >-
          List of Evaluator Logs associated with the Log. These contain
          Evaluator judgments on the Log.
        type: list<EvaluatorLogResponse>
      evaluator:
        type: EvaluatorResponse
        docs: The Evaluator used to generate the judgment.
      parent:
        type: optional<LogResponse>
        docs: >-
          The Log that was evaluated. Only provided if the ?include_parent query
          parameter is set for the 
    source:
      openapi: openapi/openapi.auto.json
  EvaluatorResponseSpec:
    discriminated: false
    union:
      - LlmEvaluatorRequest
      - CodeEvaluatorRequest
      - HumanEvaluatorRequest
      - ExternalEvaluatorRequest
    source:
      openapi: openapi/openapi.auto.json
  EvaluatorResponse:
    docs: Version of the Evaluator used to provide judgments.
    properties:
      path:
        type: string
        docs: >-
          Path of the Evaluator including the Evaluator name, which is used as a
          unique identifier.
      id:
        type: string
        docs: Unique identifier for the Evaluator.
      directory_id:
        type: optional<string>
        docs: ID of the directory that the file is in on Humanloop.
      commit_message:
        type: optional<string>
        docs: Message describing the changes made.
      spec: EvaluatorResponseSpec
      name:
        type: string
        docs: Name of the Evaluator, which is used as a unique identifier.
      version_id:
        type: string
        docs: >-
          Unique identifier for the specific Evaluator Version. If no query
          params provided, the default deployed Evaluator Version is returned.
      type: optional<literal<"evaluator">>
      environments:
        type: optional<list<EnvironmentResponse>>
        docs: The list of environments the Prompt Version is deployed to.
      created_at: datetime
      updated_at: datetime
      created_by:
        type: optional<UserResponse>
        docs: The user who created the Prompt.
      status: VersionStatus
      last_used_at: datetime
      version_logs_count:
        type: integer
        docs: The number of logs that have been generated for this Prompt Version
      total_logs_count:
        type: integer
        docs: The number of logs that have been generated across all Prompt Versions
      inputs:
        docs: >-
          Inputs associated to the Prompt. Inputs correspond to any of the
          variables used within the Prompt template.
        type: list<InputResponse>
      evaluators:
        type: optional<list<MonitoringEvaluatorResponse>>
        docs: >-
          Evaluators that have been attached to this Prompt that are used for
          monitoring logs.
      evaluator_aggregates:
        type: optional<list<EvaluatorAggregate>>
        docs: Aggregation of Evaluator results for the Evaluator Version.
    source:
      openapi: openapi/openapi.auto.json
  EvaluatorReturnTypeEnum:
    enum:
      - boolean
      - number
      - select
      - multi_select
      - text
    docs: Enum representing the possible return types of an evaluator.
    source:
      openapi: openapi/openapi.auto.json
  ExternalEvaluatorRequest:
    properties:
      arguments_type:
        type: EvaluatorArgumentsType
        docs: Whether this evaluator is target-free or target-required.
      return_type:
        type: EvaluatorReturnTypeEnum
        docs: The type of the return value of the evaluator.
      evaluator_type: literal<"external">
      metadata:
        type: optional<map<string, unknown>>
        docs: Metadata describing the external Evaluator.
    source:
      openapi: openapi/openapi.auto.json
  FileEnvironmentResponseFile:
    discriminated: false
    docs: >-
      The version of the File that is deployed to the Environment, if one is
      deployed.
    union:
      - PromptResponse
      - ToolResponse
      - DatasetResponse
      - EvaluatorResponse
    source:
      openapi: openapi/openapi.auto.json
  FileEnvironmentResponse:
    docs: >-
      Response model for the List Environments endpoint under Files.


      Contains the deployed version of the File, if one is deployed to the
      Environment.
    properties:
      id: string
      created_at: datetime
      name: string
      tag: EnvironmentTag
      file:
        type: optional<FileEnvironmentResponseFile>
        docs: >-
          The version of the File that is deployed to the Environment, if one is
          deployed.
    source:
      openapi: openapi/openapi.auto.json
  FunctionTool:
    docs: A function tool to be called by the model where user owns runtime.
    properties:
      name: string
      arguments: optional<string>
    source:
      openapi: openapi/openapi.auto.json
  FunctionToolChoice:
    docs: A function tool to be called by the model where user owns runtime.
    properties:
      name: string
    source:
      openapi: openapi/openapi.auto.json
  HttpValidationError:
    properties:
      detail: optional<list<ValidationError>>
    source:
      openapi: openapi/openapi.auto.json
  HumanEvaluatorRequestReturnType:
    enum:
      - select
      - multi_select
      - text
      - number
      - boolean
    docs: The type of the return value of the Evaluator.
    source:
      openapi: openapi/openapi.auto.json
  HumanEvaluatorRequest:
    properties:
      arguments_type:
        type: EvaluatorArgumentsType
        docs: Whether this evaluator is target-free or target-required.
      return_type:
        type: HumanEvaluatorRequestReturnType
        docs: The type of the return value of the Evaluator.
      evaluator_type: literal<"human">
      instructions:
        type: optional<string>
        docs: Instructions for the Human annotating the .
      options:
        type: optional<list<EvaluatorJudgmentOptionResponse>>
        docs: The options that the Human annotator can choose from.
      number_limits:
        type: optional<EvaluatorJudgmentNumberLimit>
        docs: >-
          Limits on the judgment that can be applied. Only for Evaluators with
          `return_type` of `'number'`.
      number_valence:
        type: optional<Valence>
        docs: >-
          The valence of the number judgment. Only for Evaluators with
          `return_type` of `'number'`. If 'positive', a higher number is better.
          If 'negative', a lower number is better.
    source:
      openapi: openapi/openapi.auto.json
  ImageChatContent:
    properties:
      type: literal<"image_url">
      image_url:
        type: ImageUrl
        docs: The message's image content.
    source:
      openapi: openapi/openapi.auto.json
  ImageUrlDetail:
    enum:
      - high
      - low
      - auto
    docs: >-
      Specify the detail level of the image provided to the model. For more
      details see:
      https://platform.openai.com/docs/guides/vision/low-or-high-fidelity-image-understanding
    source:
      openapi: openapi/openapi.auto.json
  ImageUrl:
    properties:
      url:
        type: string
        docs: Either a URL of the image or the base64 encoded image data.
      detail:
        type: optional<ImageUrlDetail>
        docs: >-
          Specify the detail level of the image provided to the model. For more
          details see:
          https://platform.openai.com/docs/guides/vision/low-or-high-fidelity-image-understanding
    source:
      openapi: openapi/openapi.auto.json
  InputResponse:
    properties:
      name:
        type: string
        docs: Type of input.
    source:
      openapi: openapi/openapi.auto.json
  LlmEvaluatorRequest:
    properties:
      arguments_type:
        type: EvaluatorArgumentsType
        docs: Whether this evaluator is target-free or target-required.
      return_type:
        type: EvaluatorReturnTypeEnum
        docs: The type of the return value of the evaluator.
      evaluator_type: literal<"llm">
      prompt:
        type: optional<PromptKernelRequest>
        docs: The prompt parameters used to generate.
    source:
      openapi: openapi/openapi.auto.json
  LinkedToolResponse:
    properties:
      name:
        type: string
        docs: Name for the tool referenced by the model.
      description:
        type: string
        docs: Description of the tool referenced by the model
      strict:
        type: optional<boolean>
        docs: >-
          If true, forces the model to output json data in the structure of the
          parameters schema.
        default: false
      parameters:
        type: optional<map<string, unknown>>
        docs: >-
          Parameters needed to run the Tool, defined in JSON Schema format:
          https://json-schema.org/
      id:
        type: string
        docs: Unique identifier for the Tool linked.
      version_id:
        type: string
        docs: Unique identifier for the Tool Version linked.
    source:
      openapi: openapi/openapi.auto.json
  ListDatasets:
    properties:
      records:
        docs: The list of Datasets.
        type: list<DatasetResponse>
    source:
      openapi: openapi/openapi.auto.json
  ListEvaluators:
    properties:
      records:
        docs: The list of Evaluators.
        type: list<EvaluatorResponse>
    source:
      openapi: openapi/openapi.auto.json
  ListPrompts:
    properties:
      records:
        docs: The list of Prompts.
        type: list<PromptResponse>
    source:
      openapi: openapi/openapi.auto.json
  ListTools:
    properties:
      records:
        docs: The list of Tools.
        type: list<ToolResponse>
    source:
      openapi: openapi/openapi.auto.json
  LogResponse:
    discriminated: false
    union:
      - PromptLogResponse
      - ToolLogResponse
      - EvaluatorLogResponse
    source:
      openapi: openapi/openapi.auto.json
  ModelEndpoints:
    enum:
      - complete
      - chat
      - edit
    docs: Supported model provider endpoints.
    source:
      openapi: openapi/openapi.auto.json
  ModelProviders:
    enum:
      - openai
      - openai_azure
      - mock
      - anthropic
      - cohere
      - replicate
      - google
      - groq
    docs: Supported model providers.
    source:
      openapi: openapi/openapi.auto.json
  MonitoringEvaluatorEnvironmentRequest:
    properties:
      evaluator_id:
        type: string
        docs: Unique identifier for the Evaluator to be used for monitoring.
      environment_id:
        type: string
        docs: >-
          Unique identifier for the Environment. The Evaluator Version deployed
          to this Environment will be used for monitoring.
    source:
      openapi: openapi/openapi.auto.json
  MonitoringEvaluatorResponse:
    properties:
      version_reference:
        type: VersionReferenceResponse
        docs: >-
          The Evaluator Version used for monitoring. This can be a specific
          Version by ID, or a Version deployed to an Environment.
      version:
        type: optional<EvaluatorResponse>
        docs: The deployed Version.
      state:
        type: MonitoringEvaluatorState
        docs: The state of the Monitoring Evaluator. Either `active` or `inactive`
      created_at: datetime
      updated_at: datetime
    source:
      openapi: openapi/openapi.auto.json
  MonitoringEvaluatorState:
    enum:
      - active
      - inactive
    docs: State of an evaluator connected to a file
    source:
      openapi: openapi/openapi.auto.json
  MonitoringEvaluatorVersionRequest:
    properties:
      evaluator_version_id:
        type: string
        docs: Unique identifier for the Evaluator Version to be used for monitoring.
    source:
      openapi: openapi/openapi.auto.json
  NumericEvaluatorVersionStats:
    docs: |-
      Base attributes for stats for an Evaluator Version-Evaluated Version pair
      in the Evaluation Report.
    properties:
      evaluator_version_id:
        type: string
        docs: Unique identifier for the Evaluator Version.
      total_logs:
        type: integer
        docs: >-
          The total number of Logs generated by this Evaluator Version on the
          Evaluated Version's Logs. This includes Nulls and Errors.
      num_judgments:
        type: integer
        docs: >-
          The total number of Evaluator judgments for this Evaluator Version.
          This excludes Nulls and Errors.
      num_nulls:
        type: integer
        docs: >-
          The total number of null judgments (i.e. abstentions) for this
          Evaluator Version.
      num_errors:
        type: integer
        docs: The total number of errored Evaluators for this Evaluator Version.
      mean: optional<double>
      std: optional<double>
      percentiles: map<string, double>
    source:
      openapi: openapi/openapi.auto.json
  ObservabilityStatus:
    enum:
      - pending
      - running
      - completed
      - failed
    docs: |-
      Status of a Log for observability.

      Observability is implemented by running monitoring Evaluators on Logs.
    source:
      openapi: openapi/openapi.auto.json
  OverallStats:
    properties:
      num_datapoints:
        type: integer
        docs: >-
          The total number of Datapoints in the Evaluation Report's Dataset
          Version.
      total_logs:
        type: integer
        docs: The total number of Logs in the Evaluation Report.
      total_evaluator_logs:
        type: integer
        docs: The total number of Evaluator Logs in the Evaluation Report.
    source:
      openapi: openapi/openapi.auto.json
  PaginatedDatapointResponse:
    properties:
      records: list<DatapointResponse>
      page: integer
      size: integer
      total: integer
    source:
      openapi: openapi/openapi.auto.json
  PaginatedDatasetResponse:
    properties:
      records: list<DatasetResponse>
      page: integer
      size: integer
      total: integer
    source:
      openapi: openapi/openapi.auto.json
  PaginatedDataEvaluationReportLogResponse:
    properties:
      records: list<EvaluationReportLogResponse>
      page: integer
      size: integer
      total: integer
    source:
      openapi: openapi/openapi.auto.json
  PaginatedEvaluationResponse:
    properties:
      records: list<EvaluationResponse>
      page: integer
      size: integer
      total: integer
    source:
      openapi: openapi/openapi.auto.json
  PaginatedDataEvaluatorResponse:
    properties:
      records: list<EvaluatorResponse>
      page: integer
      size: integer
      total: integer
    source:
      openapi: openapi/openapi.auto.json
  PaginatedDataLogResponse:
    properties:
      records: list<LogResponse>
      page: integer
      size: integer
      total: integer
    source:
      openapi: openapi/openapi.auto.json
  PaginatedDataPromptResponse:
    properties:
      records: list<PromptResponse>
      page: integer
      size: integer
      total: integer
    source:
      openapi: openapi/openapi.auto.json
  PaginatedSessionResponse:
    properties:
      records: list<SessionResponse>
      page: integer
      size: integer
      total: integer
    source:
      openapi: openapi/openapi.auto.json
  PaginatedDataToolResponse:
    properties:
      records: list<ToolResponse>
      page: integer
      size: integer
      total: integer
    source:
      openapi: openapi/openapi.auto.json
  PlatformAccessEnum:
    enum:
      - superadmin
      - supportadmin
      - user
    docs: An enumeration.
    source:
      openapi: openapi/openapi.auto.json
  ProjectSortBy:
    enum:
      - created_at
      - updated_at
      - name
    docs: An enumeration.
    source:
      openapi: openapi/openapi.auto.json
  PromptCallLogResponse:
    docs: Sample specific response details for a Prompt call
    properties:
      output:
        type: optional<string>
        docs: >-
          Generated output from your model for the provided inputs. Can be
          `None` if logging an error, or if creating a parent Log with the
          intention to populate it later.
      created_at:
        type: optional<datetime>
        docs: 'User defined timestamp for when the log was created. '
      error:
        type: optional<string>
        docs: Error message if the log is an error.
      provider_latency:
        type: optional<double>
        docs: Duration of the logged event in seconds.
      stdout:
        type: optional<string>
        docs: Captured log and debug statements.
      output_message:
        type: optional<ChatMessage>
        docs: The message returned by the provider.
      prompt_tokens:
        type: optional<integer>
        docs: Number of tokens in the prompt used to generate the output.
      output_tokens:
        type: optional<integer>
        docs: Number of tokens in the output generated by the model.
      prompt_cost:
        type: optional<double>
        docs: Cost in dollars associated to the tokens in the prompt.
      output_cost:
        type: optional<double>
        docs: Cost in dollars associated to the tokens in the output.
      finish_reason:
        type: optional<string>
        docs: Reason the generation finished.
      index:
        type: integer
        docs: The index of the sample in the batch.
    source:
      openapi: openapi/openapi.auto.json
  PromptCallResponseToolChoice:
    discriminated: false
    docs: >-
      Controls how the model uses tools. The following options are supported: 

      - `'none'` means the model will not call any tool and instead generates a
      message; this is the default when no tools are provided as part of the
      Prompt. 

      - `'auto'` means the model can decide to call one or more of the provided
      tools; this is the default when tools are provided as part of the Prompt. 

      - `'required'` means the model can decide to call one or more of the
      provided tools. 

      - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the
      model to use the named function.
    union:
      - literal<"none">
      - literal<"auto">
      - literal<"required">
      - ToolChoice
    source:
      openapi: openapi/openapi.auto.json
  PromptCallResponse:
    docs: Response model for a Prompt call with potentially multiple log samples.
    properties:
      prompt:
        type: PromptResponse
        docs: Prompt details used to generate the log.
      messages:
        type: optional<list<ChatMessage>>
        docs: The messages passed to the to provider chat endpoint.
      tool_choice:
        type: optional<PromptCallResponseToolChoice>
        docs: >-
          Controls how the model uses tools. The following options are
          supported: 

          - `'none'` means the model will not call any tool and instead
          generates a message; this is the default when no tools are provided as
          part of the Prompt. 

          - `'auto'` means the model can decide to call one or more of the
          provided tools; this is the default when tools are provided as part of
          the Prompt. 

          - `'required'` means the model can decide to call one or more of the
          provided tools. 

          - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the
          model to use the named function.
      session_id:
        type: optional<string>
        docs: >-
          Unique identifier for the Session to associate the Log to. Allows you
          to record multiple Logs to a Session (using an ID kept by your
          internal systems) by passing the same `session_id` in subsequent log
          requests. 
      parent_id:
        type: optional<string>
        docs: >-
          Unique identifier for the parent Log in a Session. Should only be
          provided if `session_id` is provided. If provided, the Log will be
          nested under the parent Log within the Session.
      inputs:
        type: optional<map<string, unknown>>
        docs: The inputs passed to the prompt template.
      source:
        type: optional<string>
        docs: Identifies where the model was called from.
      metadata:
        type: optional<map<string, unknown>>
        docs: Any additional metadata to record.
      save:
        type: optional<boolean>
        docs: Whether the request/response payloads will be stored on Humanloop.
        default: true
      source_datapoint_id:
        type: optional<string>
        docs: >-
          Unique identifier for the Datapoint that this Log is derived from.
          This can be used by Humanloop to associate Logs to Evaluations. If
          provided, Humanloop will automatically associate this Log to
          Evaluations that require a Log for this Datapoint-Version pair.
      batches:
        type: optional<list<string>>
        docs: >-
          Array of Batch Ids that this log is part of. Batches are used to group
          Logs together for offline Evaluations
      user:
        type: optional<string>
        docs: End-user ID related to the Log.
      environment:
        type: optional<string>
        docs: The name of the Environment the Log is associated to.
      id:
        type: string
        docs: ID of the log.
      logs:
        docs: The logs generated by the Prompt call.
        type: list<PromptCallLogResponse>
    source:
      openapi: openapi/openapi.auto.json
  PromptCallStreamResponse:
    docs: Response model for calling Prompt in streaming mode.
    properties:
      output:
        type: optional<string>
        docs: >-
          Generated output from your model for the provided inputs. Can be
          `None` if logging an error, or if creating a parent Log with the
          intention to populate it later.
      created_at:
        type: optional<datetime>
        docs: 'User defined timestamp for when the log was created. '
      error:
        type: optional<string>
        docs: Error message if the log is an error.
      provider_latency:
        type: optional<double>
        docs: Duration of the logged event in seconds.
      stdout:
        type: optional<string>
        docs: Captured log and debug statements.
      output_message:
        type: optional<ChatMessage>
        docs: The message returned by the provider.
      prompt_tokens:
        type: optional<integer>
        docs: Number of tokens in the prompt used to generate the output.
      output_tokens:
        type: optional<integer>
        docs: Number of tokens in the output generated by the model.
      prompt_cost:
        type: optional<double>
        docs: Cost in dollars associated to the tokens in the prompt.
      output_cost:
        type: optional<double>
        docs: Cost in dollars associated to the tokens in the output.
      finish_reason:
        type: optional<string>
        docs: Reason the generation finished.
      index:
        type: integer
        docs: The index of the sample in the batch.
      id:
        type: string
        docs: ID of the log.
      prompt_id:
        type: string
        docs: ID of the Prompt the log belongs to.
      version_id:
        type: string
        docs: ID of the specific version of the Prompt.
    source:
      openapi: openapi/openapi.auto.json
  PromptKernelRequestTemplate:
    discriminated: false
    docs: >-
      For chat endpoint, provide a Chat template. For completion endpoint,
      provide a Prompt template. Input variables within the template should be
      specified with double curly bracket syntax: {{INPUT_NAME}}.
    union:
      - string
      - list<ChatMessage>
    source:
      openapi: openapi/openapi.auto.json
  PromptKernelRequestStop:
    discriminated: false
    docs: >-
      The string (or list of strings) after which the model will stop
      generating. The returned text will not contain the stop sequence.
    union:
      - string
      - list<string>
    source:
      openapi: openapi/openapi.auto.json
  PromptKernelRequest:
    properties:
      model:
        type: string
        docs: >-
          The model instance used, e.g. `gpt-4`. See [supported
          models](https://humanloop.com/docs/supported-models)
      endpoint:
        type: optional<ModelEndpoints>
        docs: The provider model endpoint used.
      template:
        type: optional<PromptKernelRequestTemplate>
        docs: >-
          For chat endpoint, provide a Chat template. For completion endpoint,
          provide a Prompt template. Input variables within the template should
          be specified with double curly bracket syntax: {{INPUT_NAME}}.
      provider:
        type: optional<ModelProviders>
        docs: The company providing the underlying model service.
      max_tokens:
        type: optional<integer>
        docs: >-
          The maximum number of tokens to generate. Provide max_tokens=-1 to
          dynamically calculate the maximum number of tokens to generate given
          the length of the prompt
        default: -1
      temperature:
        type: optional<double>
        docs: >-
          What sampling temperature to use when making a generation. Higher
          values means the model will be more creative.
        default: 1
      top_p:
        type: optional<double>
        docs: >-
          An alternative to sampling with temperature, called nucleus sampling,
          where the model considers the results of the tokens with top_p
          probability mass.
        default: 1
      stop:
        type: optional<PromptKernelRequestStop>
        docs: >-
          The string (or list of strings) after which the model will stop
          generating. The returned text will not contain the stop sequence.
      presence_penalty:
        type: optional<double>
        docs: >-
          Number between -2.0 and 2.0. Positive values penalize new tokens based
          on whether they appear in the generation so far.
        default: 0
      frequency_penalty:
        type: optional<double>
        docs: >-
          Number between -2.0 and 2.0. Positive values penalize new tokens based
          on how frequently they appear in the generation so far.
        default: 0
      other:
        type: optional<map<string, unknown>>
        docs: Other parameter values to be passed to the provider call.
      seed:
        type: optional<integer>
        docs: >-
          If specified, model will make a best effort to sample
          deterministically, but it is not guaranteed.
      response_format:
        type: optional<ResponseFormat>
        docs: >-
          The format of the response. Only `{"type": "json_object"}` is
          currently supported for chat.
      tools:
        type: optional<list<ToolFunction>>
        docs: >-
          The tool specification that the model can choose to call if Tool
          calling is supported.
      linked_tools:
        type: optional<list<string>>
        docs: >-
          The IDs of the Tools in your organization that the model can choose to
          call if Tool calling is supported. The default deployed version of
          that tool is called.
    source:
      openapi: openapi/openapi.auto.json
  PromptLogResponseToolChoice:
    discriminated: false
    docs: >-
      Controls how the model uses tools. The following options are supported: 

      - `'none'` means the model will not call any tool and instead generates a
      message; this is the default when no tools are provided as part of the
      Prompt. 

      - `'auto'` means the model can decide to call one or more of the provided
      tools; this is the default when tools are provided as part of the Prompt. 

      - `'required'` means the model can decide to call one or more of the
      provided tools. 

      - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the
      model to use the named function.
    union:
      - literal<"none">
      - literal<"auto">
      - literal<"required">
      - ToolChoice
    source:
      openapi: openapi/openapi.auto.json
  PromptLogResponse:
    docs: General request for creating a Log
    properties:
      output_message:
        type: optional<ChatMessage>
        docs: The message returned by the provider.
      prompt_tokens:
        type: optional<integer>
        docs: Number of tokens in the prompt used to generate the output.
      output_tokens:
        type: optional<integer>
        docs: Number of tokens in the output generated by the model.
      prompt_cost:
        type: optional<double>
        docs: Cost in dollars associated to the tokens in the prompt.
      output_cost:
        type: optional<double>
        docs: Cost in dollars associated to the tokens in the output.
      finish_reason:
        type: optional<string>
        docs: Reason the generation finished.
      prompt:
        type: PromptResponse
        docs: Prompt details used to generate the Log.
      messages:
        type: optional<list<ChatMessage>>
        docs: The messages passed to the to provider chat endpoint.
      tool_choice:
        type: optional<PromptLogResponseToolChoice>
        docs: >-
          Controls how the model uses tools. The following options are
          supported: 

          - `'none'` means the model will not call any tool and instead
          generates a message; this is the default when no tools are provided as
          part of the Prompt. 

          - `'auto'` means the model can decide to call one or more of the
          provided tools; this is the default when tools are provided as part of
          the Prompt. 

          - `'required'` means the model can decide to call one or more of the
          provided tools. 

          - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the
          model to use the named function.
      output:
        type: optional<string>
        docs: >-
          Generated output from your model for the provided inputs. Can be
          `None` if logging an error, or if creating a parent Log with the
          intention to populate it later.
      created_at:
        type: optional<datetime>
        docs: 'User defined timestamp for when the log was created. '
      error:
        type: optional<string>
        docs: Error message if the log is an error.
      provider_latency:
        type: optional<double>
        docs: Duration of the logged event in seconds.
      stdout:
        type: optional<string>
        docs: Captured log and debug statements.
      provider_request:
        type: optional<map<string, unknown>>
        docs: Raw request sent to provider.
      provider_response:
        type: optional<map<string, unknown>>
        docs: Raw response received the provider.
      session_id:
        type: optional<string>
        docs: >-
          Unique identifier for the Session to associate the Log to. Allows you
          to record multiple Logs to a Session (using an ID kept by your
          internal systems) by passing the same `session_id` in subsequent log
          requests. 
      parent_id:
        type: optional<string>
        docs: >-
          Unique identifier for the parent Log in a Session. Should only be
          provided if `session_id` is provided. If provided, the Log will be
          nested under the parent Log within the Session.
      inputs:
        type: optional<map<string, unknown>>
        docs: The inputs passed to the prompt template.
      source:
        type: optional<string>
        docs: Identifies where the model was called from.
      metadata:
        type: optional<map<string, unknown>>
        docs: Any additional metadata to record.
      save:
        type: optional<boolean>
        docs: Whether the request/response payloads will be stored on Humanloop.
        default: true
      source_datapoint_id:
        type: optional<string>
        docs: >-
          Unique identifier for the Datapoint that this Log is derived from.
          This can be used by Humanloop to associate Logs to Evaluations. If
          provided, Humanloop will automatically associate this Log to
          Evaluations that require a Log for this Datapoint-Version pair.
      batches:
        type: optional<list<string>>
        docs: >-
          Array of Batch Ids that this log is part of. Batches are used to group
          Logs together for offline Evaluations
      user:
        type: optional<string>
        docs: End-user ID related to the Log.
      environment:
        type: optional<string>
        docs: The name of the Environment the Log is associated to.
      id:
        type: string
        docs: Unique identifier for the Log.
      evaluator_logs:
        docs: >-
          List of Evaluator Logs associated with the Log. These contain
          Evaluator judgments on the Log.
        type: list<EvaluatorLogResponse>
    source:
      openapi: openapi/openapi.auto.json
  PromptResponseTemplate:
    discriminated: false
    docs: >-
      For chat endpoint, provide a Chat template. For completion endpoint,
      provide a Prompt template. Input variables within the template should be
      specified with double curly bracket syntax: {{INPUT_NAME}}.
    union:
      - string
      - list<ChatMessage>
    source:
      openapi: openapi/openapi.auto.json
  PromptResponseStop:
    discriminated: false
    docs: >-
      The string (or list of strings) after which the model will stop
      generating. The returned text will not contain the stop sequence.
    union:
      - string
      - list<string>
    source:
      openapi: openapi/openapi.auto.json
  PromptResponse:
    docs: >-
      Base type that all File Responses should inherit from.


      Attributes defined here are common to all File Responses and should be
      overridden

      in the inheriting classes with documentation and appropriate Field
      definitions.
    properties:
      path:
        type: string
        docs: >-
          Path of the Prompt, including the name, which is used as a unique
          identifier.
      id:
        type: string
        docs: Unique identifier for the Prompt.
      directory_id:
        type: optional<string>
        docs: ID of the directory that the file is in on Humanloop.
      model:
        type: string
        docs: >-
          The model instance used, e.g. `gpt-4`. See [supported
          models](https://humanloop.com/docs/supported-models)
      endpoint:
        type: optional<ModelEndpoints>
        docs: The provider model endpoint used.
      template:
        type: optional<PromptResponseTemplate>
        docs: >-
          For chat endpoint, provide a Chat template. For completion endpoint,
          provide a Prompt template. Input variables within the template should
          be specified with double curly bracket syntax: {{INPUT_NAME}}.
      provider:
        type: optional<ModelProviders>
        docs: The company providing the underlying model service.
      max_tokens:
        type: optional<integer>
        docs: >-
          The maximum number of tokens to generate. Provide max_tokens=-1 to
          dynamically calculate the maximum number of tokens to generate given
          the length of the prompt
        default: -1
      temperature:
        type: optional<double>
        docs: >-
          What sampling temperature to use when making a generation. Higher
          values means the model will be more creative.
        default: 1
      top_p:
        type: optional<double>
        docs: >-
          An alternative to sampling with temperature, called nucleus sampling,
          where the model considers the results of the tokens with top_p
          probability mass.
        default: 1
      stop:
        type: optional<PromptResponseStop>
        docs: >-
          The string (or list of strings) after which the model will stop
          generating. The returned text will not contain the stop sequence.
      presence_penalty:
        type: optional<double>
        docs: >-
          Number between -2.0 and 2.0. Positive values penalize new tokens based
          on whether they appear in the generation so far.
        default: 0
      frequency_penalty:
        type: optional<double>
        docs: >-
          Number between -2.0 and 2.0. Positive values penalize new tokens based
          on how frequently they appear in the generation so far.
        default: 0
      other:
        type: optional<map<string, unknown>>
        docs: Other parameter values to be passed to the provider call.
      seed:
        type: optional<integer>
        docs: >-
          If specified, model will make a best effort to sample
          deterministically, but it is not guaranteed.
      response_format:
        type: optional<ResponseFormat>
        docs: >-
          The format of the response. Only `{"type": "json_object"}` is
          currently supported for chat.
      tools:
        type: optional<list<ToolFunction>>
        docs: >-
          The tool specification that the model can choose to call if Tool
          calling is supported.
      linked_tools:
        type: optional<list<LinkedToolResponse>>
        docs: The tools linked to your prompt that the model can call.
      commit_message:
        type: optional<string>
        docs: Message describing the changes made.
      name:
        type: string
        docs: Name of the Prompt.
      version_id:
        type: string
        docs: >-
          Unique identifier for the specific Prompt Version. If no query params
          provided, the default deployed Prompt Version is returned.
      type: optional<literal<"prompt">>
      environments:
        type: optional<list<EnvironmentResponse>>
        docs: The list of environments the Prompt Version is deployed to.
      created_at: datetime
      updated_at: datetime
      created_by:
        type: optional<UserResponse>
        docs: The user who created the Prompt.
      status:
        type: VersionStatus
        docs: The status of the Prompt Version.
      last_used_at: datetime
      version_logs_count:
        type: integer
        docs: The number of logs that have been generated for this Prompt Version
      total_logs_count:
        type: integer
        docs: The number of logs that have been generated across all Prompt Versions
      inputs:
        docs: >-
          Inputs associated to the Prompt. Inputs correspond to any of the
          variables used within the Prompt template.
        type: list<InputResponse>
      evaluators:
        type: optional<list<MonitoringEvaluatorResponse>>
        docs: >-
          Evaluators that have been attached to this Prompt that are used for
          monitoring logs.
      evaluator_aggregates:
        type: optional<list<EvaluatorAggregate>>
        docs: Aggregation of Evaluator results for the Prompt Version.
    source:
      openapi: openapi/openapi.auto.json
  ProviderApiKeys:
    properties:
      openai: optional<string>
      ai21: optional<string>
      mock: optional<string>
      anthropic: optional<string>
      cohere: optional<string>
      openai_azure: optional<string>
      openai_azure_endpoint: optional<string>
    source:
      openapi: openapi/openapi.auto.json
  ResponseFormatType:
    enum:
      - json_object
      - json_schema
    source:
      openapi: openapi/openapi.auto.json
  ResponseFormat:
    docs: Response format of the model.
    properties:
      type: ResponseFormatType
      json_schema:
        type: optional<map<string, unknown>>
        docs: The JSON schema of the response format if type is json_schema.
    source:
      openapi: openapi/openapi.auto.json
  SelectEvaluatorVersionStats:
    docs: Also used for 'multi_select' Evaluator versions
    properties:
      evaluator_version_id:
        type: string
        docs: Unique identifier for the Evaluator Version.
      total_logs:
        type: integer
        docs: >-
          The total number of Logs generated by this Evaluator Version on the
          Evaluated Version's Logs. This includes Nulls and Errors.
      num_judgments:
        type: integer
        docs: >-
          The total number of Evaluator judgments for this Evaluator Version.
          This excludes Nulls and Errors.
      num_nulls:
        type: integer
        docs: >-
          The total number of null judgments (i.e. abstentions) for this
          Evaluator Version.
      num_errors:
        type: integer
        docs: The total number of errored Evaluators for this Evaluator Version.
      num_judgments_per_option:
        type: map<string, integer>
        docs: >-
          The total number of Evaluator judgments for this Evaluator Version.
          This is a mapping of the option name to the number of judgments for
          that option.
    source:
      openapi: openapi/openapi.auto.json
  SessionEventResponse:
    properties:
      log: LogResponse
      children: list<SessionEventResponse>
    source:
      openapi: openapi/openapi.auto.json
  SessionResponse:
    properties:
      id:
        type: string
        docs: Unique identifier for the Session.
      created_at: datetime
      updated_at: datetime
      first_inputs:
        type: optional<map<string, unknown>>
        docs: Inputs for the first datapoint in the session.
      last_output:
        type: optional<string>
        docs: Output for the last datapoint in the session.
      logs_count:
        type: integer
        docs: Number of logs associated to this session.
      events:
        docs: List of events associated with this Session.
        type: list<SessionEventResponse>
    source:
      openapi: openapi/openapi.auto.json
  SortOrder:
    enum:
      - asc
      - desc
    docs: An enumeration.
    source:
      openapi: openapi/openapi.auto.json
  TextChatContent:
    properties:
      type: literal<"text">
      text:
        type: string
        docs: The message's text content.
    source:
      openapi: openapi/openapi.auto.json
  TextEvaluatorVersionStats:
    docs: |-
      Base attributes for stats for an Evaluator Version-Evaluated Version pair
      in the Evaluation Report.
    properties:
      evaluator_version_id:
        type: string
        docs: Unique identifier for the Evaluator Version.
      total_logs:
        type: integer
        docs: >-
          The total number of Logs generated by this Evaluator Version on the
          Evaluated Version's Logs. This includes Nulls and Errors.
      num_judgments:
        type: integer
        docs: >-
          The total number of Evaluator judgments for this Evaluator Version.
          This excludes Nulls and Errors.
      num_nulls:
        type: integer
        docs: >-
          The total number of null judgments (i.e. abstentions) for this
          Evaluator Version.
      num_errors:
        type: integer
        docs: The total number of errored Evaluators for this Evaluator Version.
    source:
      openapi: openapi/openapi.auto.json
  TimeUnit:
    enum:
      - day
      - week
      - month
    docs: An enumeration.
    source:
      openapi: openapi/openapi.auto.json
  ToolCall:
    docs: A tool call to be made.
    properties:
      id: string
      type: ChatToolType
      function: FunctionTool
    source:
      openapi: openapi/openapi.auto.json
  ToolChoice:
    docs: Tool choice to force the model to use a tool.
    properties:
      type: ChatToolType
      function: FunctionToolChoice
    source:
      openapi: openapi/openapi.auto.json
  ToolFunction:
    properties:
      name:
        type: string
        docs: Name for the tool referenced by the model.
      description:
        type: string
        docs: Description of the tool referenced by the model
      strict:
        type: optional<boolean>
        docs: >-
          If true, forces the model to output json data in the structure of the
          parameters schema.
        default: false
      parameters:
        type: optional<map<string, unknown>>
        docs: >-
          Parameters needed to run the Tool, defined in JSON Schema format:
          https://json-schema.org/
    source:
      openapi: openapi/openapi.auto.json
  ToolKernelRequest:
    properties:
      function:
        type: optional<ToolFunction>
        docs: >-
          Callable function specification of the Tool shown to the model for
          tool calling.
      source_code:
        type: optional<string>
        docs: Code source of the Tool.
      setup_values:
        type: optional<map<string, unknown>>
        docs: >-
          Values needed to setup the Tool, defined in JSON Schema format:
          https://json-schema.org/
    source:
      openapi: openapi/openapi.auto.json
  ToolLogResponse:
    docs: General request for creating a Log
    properties:
      output:
        type: optional<string>
        docs: >-
          Generated output from your model for the provided inputs. Can be
          `None` if logging an error, or if creating a parent Log with the
          intention to populate it later.
      created_at:
        type: optional<datetime>
        docs: 'User defined timestamp for when the log was created. '
      error:
        type: optional<string>
        docs: Error message if the log is an error.
      provider_latency:
        type: optional<double>
        docs: Duration of the logged event in seconds.
      stdout:
        type: optional<string>
        docs: Captured log and debug statements.
      provider_request:
        type: optional<map<string, unknown>>
        docs: Raw request sent to provider.
      provider_response:
        type: optional<map<string, unknown>>
        docs: Raw response received the provider.
      session_id:
        type: optional<string>
        docs: >-
          Unique identifier for the Session to associate the Log to. Allows you
          to record multiple Logs to a Session (using an ID kept by your
          internal systems) by passing the same `session_id` in subsequent log
          requests. 
      parent_id:
        type: optional<string>
        docs: >-
          Unique identifier for the parent Log in a Session. Should only be
          provided if `session_id` is provided. If provided, the Log will be
          nested under the parent Log within the Session.
      inputs:
        type: optional<map<string, unknown>>
        docs: The inputs passed to the prompt template.
      source:
        type: optional<string>
        docs: Identifies where the model was called from.
      metadata:
        type: optional<map<string, unknown>>
        docs: Any additional metadata to record.
      save:
        type: optional<boolean>
        docs: Whether the request/response payloads will be stored on Humanloop.
        default: true
      source_datapoint_id:
        type: optional<string>
        docs: >-
          Unique identifier for the Datapoint that this Log is derived from.
          This can be used by Humanloop to associate Logs to Evaluations. If
          provided, Humanloop will automatically associate this Log to
          Evaluations that require a Log for this Datapoint-Version pair.
      batches:
        type: optional<list<string>>
        docs: >-
          Array of Batch Ids that this log is part of. Batches are used to group
          Logs together for offline Evaluations
      user:
        type: optional<string>
        docs: End-user ID related to the Log.
      environment:
        type: optional<string>
        docs: The name of the Environment the Log is associated to.
      id:
        type: string
        docs: Unique identifier for the Log.
      evaluator_logs:
        docs: >-
          List of Evaluator Logs associated with the Log. These contain
          Evaluator judgments on the Log.
        type: list<EvaluatorLogResponse>
      tool:
        type: ToolResponse
        docs: Tool details used to generate the Log.
    source:
      openapi: openapi/openapi.auto.json
  ToolResponse: unknown
  UpdateDatesetAction:
    enum:
      - set
      - add
      - remove
    docs: An enumeration.
    source:
      openapi: openapi/openapi.auto.json
  UserResponse: unknown
  Valence:
    enum:
      - positive
      - negative
      - neutral
    docs: An enumeration.
    source:
      openapi: openapi/openapi.auto.json
  ValidationErrorLocItem:
    discriminated: false
    union:
      - string
      - integer
    source:
      openapi: openapi/openapi.auto.json
  ValidationError:
    properties:
      loc: list<ValidationErrorLocItem>
      msg: string
      type: string
    source:
      openapi: openapi/openapi.auto.json
  VersionDeploymentResponseFile:
    discriminated: false
    docs: The File that the deployed Version belongs to.
    union:
      - PromptResponse
      - ToolResponse
      - DatasetResponse
      - EvaluatorResponse
    source:
      openapi: openapi/openapi.auto.json
  VersionDeploymentResponse:
    docs: A variable reference to the Version deployed to an Environment
    properties:
      file:
        type: VersionDeploymentResponseFile
        docs: The File that the deployed Version belongs to.
      environment:
        type: EnvironmentResponse
        docs: The Environment that the Version is deployed to.
      type: literal<"environment">
    source:
      openapi: openapi/openapi.auto.json
  VersionIdResponseVersion:
    discriminated: false
    docs: The specific Version being referenced.
    union:
      - PromptResponse
      - ToolResponse
      - DatasetResponse
      - EvaluatorResponse
    source:
      openapi: openapi/openapi.auto.json
  VersionIdResponse:
    docs: A reference to a specific Version by its ID
    properties:
      version:
        type: VersionIdResponseVersion
        docs: The specific Version being referenced.
      type: literal<"version">
    source:
      openapi: openapi/openapi.auto.json
  VersionReferenceResponse:
    discriminated: false
    union:
      - VersionDeploymentResponse
      - VersionIdResponse
    source:
      openapi: openapi/openapi.auto.json
  VersionStatsEvaluatorVersionStatsItem:
    discriminated: false
    union:
      - NumericEvaluatorVersionStats
      - BooleanEvaluatorVersionStats
      - SelectEvaluatorVersionStats
      - TextEvaluatorVersionStats
    source:
      openapi: openapi/openapi.auto.json
  VersionStats:
    docs: Stats for an Evaluated Version in the Evaluation Report.
    properties:
      version_id:
        type: string
        docs: Unique identifier for the Evaluated Version.
      num_logs:
        type: integer
        docs: >-
          The total number of existing Logs for this Evaluated Version within
          the Evaluation Report. These are Logs that have been generated by this
          Evaluated Version on a Datapoint belonging to the Evaluation Report's
          Dataset Version.
      evaluator_version_stats:
        docs: >-
          Stats for each Evaluator Version used to evaluate this Evaluated
          Version.
        type: list<VersionStatsEvaluatorVersionStatsItem>
    source:
      openapi: openapi/openapi.auto.json
  VersionStatus:
    enum:
      - uncommitted
      - committed
      - deleted
    docs: An enumeration.
    source:
      openapi: openapi/openapi.auto.json
  ChatToolType:
    type: literal<"function">
    docs: The type of tool to call.
  FilesToolType:
    enum:
      - pinecone_search
      - google
      - mock
      - snippet
      - json_schema
      - get_api_call
    docs: Type of tool.
    source:
      openapi: openapi/openapi.auto.json
  EvaluationsDatasetRequest:
    docs: |-
      Specification of a File version on Humanloop.

      This can be done in a couple of ways:
      - Specifying `version_id` directly.
      - Specifying a File (and optionally an Environment).
          - A File can be specified by either `path` or `file_id`.
          - An Environment can be specified by `environment_id`. If no Environment is specified, the default Environment is used.
    properties:
      version_id:
        type: optional<string>
        docs: >-
          Unique identifier for the File Version. If provided, none of the other
          fields should be specified.
      path:
        type: optional<string>
        docs: >-
          Path identifying a File. Provide either this or `file_id` if you want
          to specify a File.
      file_id:
        type: optional<string>
        docs: >-
          Unique identifier for the File. Provide either this or `path` if you
          want to specify a File.
      environment:
        type: optional<string>
        docs: >-
          Name of the Environment a Version is deployed to. Only provide this
          when specifying a File. If not provided (and a File is specified), the
          default Environment is used.
    source:
      openapi: openapi/openapi.auto.json
  EvaluationsRequest:
    docs: |-
      Specification of a File version on Humanloop.

      This can be done in a couple of ways:
      - Specifying `version_id` directly.
      - Specifying a File (and optionally an Environment).
          - A File can be specified by either `path` or `file_id`.
          - An Environment can be specified by `environment_id`. If no Environment is specified, the default Environment is used.
    properties:
      version_id:
        type: optional<string>
        docs: >-
          Unique identifier for the File Version. If provided, none of the other
          fields should be specified.
      path:
        type: optional<string>
        docs: >-
          Path identifying a File. Provide either this or `file_id` if you want
          to specify a File.
      file_id:
        type: optional<string>
        docs: >-
          Unique identifier for the File. Provide either this or `path` if you
          want to specify a File.
      environment:
        type: optional<string>
        docs: >-
          Name of the Environment a Version is deployed to. Only provide this
          when specifying a File. If not provided (and a File is specified), the
          default Environment is used.
      orchestrated:
        type: optional<boolean>
        docs: >-
          Whether the Evaluator is orchestrated by Humanloop. Default is `True`.
          If `False`, a log for the Evaluator should be submitted by the user
          via the API.
        default: true
    source:
      openapi: openapi/openapi.auto.json
  AgentConfigResponse:
    properties: {}
    source:
      openapi: openapi/openapi.auto.json
  EvaluatorConfigResponse:
    properties: {}
    source:
      openapi: openapi/openapi.auto.json
  UpdateEvaluationStatusRequest: unknown
  PaginatedPromptLogResponse: unknown
  ConfigToolResponse: unknown
  FeedbackType: unknown
  BaseModelsUserResponse: unknown
